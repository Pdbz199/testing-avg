{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1489d11b-f5a4-4036-9e45-ea39b2c2c4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actor critic agent\n",
    "# Continuous 2D\n",
    "%reset -f\n",
    "\n",
    "import torch as tor\n",
    "import matplotlib.pyplot as plt\n",
    "# Problem\n",
    "tor.manual_seed(3)\n",
    "LB = tor.tensor([[-1., -1.]]); UB = tor.tensor([[1., 1.]])\n",
    "ALB = 0.1*tor.tensor([[-.1, -.1]]); AUB = tor.tensor([[.1, .1]])\n",
    "dt = 1\n",
    "# Agent\n",
    "nhid = 10\n",
    "alpha = 0.0003\n",
    "actor_body = tor.nn.Sequential(tor.nn.Linear(4, nhid), tor.nn.ReLU(),\n",
    "                               tor.nn.Linear(nhid, nhid), tor.nn.ReLU()\n",
    "                               )\n",
    "actor_mean = tor.nn.Sequential(tor.nn.Linear(nhid, 2))\n",
    "actor_mean[-1].weight.data[:] = 0; actor_mean[-1].bias.data[:] = 0\n",
    "actor_lsigma = tor.nn.Sequential(tor.nn.Linear(nhid, 2))\n",
    "actor_lsigma[-1].weight.data[:] = 0; actor_lsigma[-1].bias.data[:] = 0\n",
    "critic = tor.nn.Sequential(tor.nn.Linear(4, nhid), tor.nn.ReLU(),\n",
    "                           tor.nn.Linear(nhid, nhid), tor.nn.ReLU(),\n",
    "                          tor.nn.Linear(nhid, 1))\n",
    "popt = tor.optim.Adam(list(actor_body.parameters())+list(actor_mean.parameters())+list(actor_lsigma.parameters()),lr=alpha)\n",
    "copt = tor.optim.Adam(critic.parameters(), lr=10*alpha)\n",
    "# Experiment\n",
    "EP = 2000\n",
    "rets = []\n",
    "Slogs = []\n",
    "i = 0\n",
    "for ep in range(EP):\n",
    "    Slogs.append([])\n",
    "    pos = tor.rand((1, 2))*(UB-LB) + LB\n",
    "    vel = tor.zeros((1, 2))\n",
    "    S = tor.cat((pos, vel), 1)\n",
    "    Slogs[-1].append(S)\n",
    "    ret = 0\n",
    "    while True:\n",
    "        # Take action\n",
    "        feat = actor_body(S)\n",
    "        mu = actor_mean(feat)\n",
    "        lsigma = actor_lsigma(feat)\n",
    "        try:\n",
    "            pol = tor.distributions.MultivariateNormal(mu, 0.01*tor.diag(tor.exp(lsigma[0])))\n",
    "        except:\n",
    "            print(\"A\")\n",
    "        A = pol.sample()\n",
    "        tor.clamp(A, ALB, AUB)\n",
    "        # Receive reward and next state\n",
    "        pos = pos + vel*dt + 0.5*A*dt**2\n",
    "        vel[pos < LB] = -0.1*vel[pos < LB]; vel[pos > UB] = -0.1*vel[pos > UB]\n",
    "        pos = tor.clamp(pos, LB, UB)\n",
    "        vel += A*dt\n",
    "        SP = tor.cat((pos, vel), 1)\n",
    "        R = -0.01\n",
    "        done = tor.allclose(pos, tor.zeros(2), atol=0.25) and tor.allclose(vel, tor.zeros(2), atol=0.1)\n",
    "        # Learning\n",
    "        vs = critic(S); vsp = critic(SP)\n",
    "        pobj = pol.log_prob(A)*(R + (1-done)*vsp - vs).detach()\n",
    "        ploss = -pobj\n",
    "        closs = (R + (1-done)*vsp.detach() - vs)**2\n",
    "        popt.zero_grad()\n",
    "        ploss.backward()\n",
    "        popt.step()\n",
    "        copt.zero_grad()\n",
    "        closs.backward()\n",
    "        copt.step()\n",
    "        # Log\n",
    "        Slogs[-1].append(SP)\n",
    "        ret += R\n",
    "        # Termination\n",
    "        if done:\n",
    "            rets.append(ret)\n",
    "            i += 1\n",
    "            print(i, len(Slogs[-1]))\n",
    "            break\n",
    "        S = SP\n",
    "# Plotting\n",
    "plt.plot(-100*tor.tensor(rets))\n",
    "plt.figure()\n",
    "colors = [\"tab:blue\", \"tab:green\", \"tab:orange\", \"tab:purple\", \"tab:red\", \"tab:brown\"]\n",
    "for i in range(-min(30, EP), 0):\n",
    "    color = colors[i%len(colors)]\n",
    "    Slog = tor.cat(Slogs[i])\n",
    "    for i in range(Slog.shape[0]-1):\n",
    "        plt.plot(Slog[i:i+2,0], Slog[i:i+2,1], alpha=(i+1)/Slog.shape[0], color=color, marker='.')\n",
    "plt.xlim([LB[0, 0], UB[0, 0]])\n",
    "plt.ylim([LB[0, 1], UB[0, 1]])\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f55ccd-5108-4b43-aeeb-64665535b01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action Value Gradient agent\n",
    "# Continuous 2D\n",
    "%reset -f\n",
    "\n",
    "import torch as tor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Problem\n",
    "tor.manual_seed(3)\n",
    "LB = tor.tensor([[-1., -1.]]); UB = tor.tensor([[1., 1.]])\n",
    "ALB = 0.1*tor.tensor([[-.1, -.1]]); AUB = tor.tensor([[.1, .1]])\n",
    "dt = 1\n",
    "\n",
    "n_timeout = 5000\n",
    "\n",
    "# Agent\n",
    "nhid = 10\n",
    "alpha = 0.0003\n",
    "actor_body = tor.nn.Sequential(tor.nn.Linear(4, nhid), tor.nn.ReLU(),\n",
    "                               tor.nn.Linear(nhid, nhid), tor.nn.ReLU()\n",
    "                               )\n",
    "actor_mean = tor.nn.Sequential(tor.nn.Linear(nhid, 2))\n",
    "actor_mean[-1].weight.data[:] = 0; actor_mean[-1].bias.data[:] = 0\n",
    "actor_lsigma = tor.nn.Sequential(tor.nn.Linear(nhid, 2))\n",
    "actor_lsigma[-1].weight.data[:] = 0; actor_lsigma[-1].bias.data[:] = 0\n",
    "q_net = tor.nn.Sequential(tor.nn.Linear(6, nhid), tor.nn.ReLU(),\n",
    "                           tor.nn.Linear(nhid, nhid), tor.nn.ReLU(),\n",
    "                          tor.nn.Linear(nhid, 1))\n",
    "popt = tor.optim.Adam(list(actor_body.parameters())+list(actor_mean.parameters())+list(actor_lsigma.parameters()),lr=alpha)\n",
    "qopt = tor.optim.Adam(q_net.parameters(), lr=10*alpha)\n",
    "\n",
    "# Experiment\n",
    "EP = 2000\n",
    "rets = []\n",
    "Slogs = []\n",
    "i = 0\n",
    "for ep in range(EP):\n",
    "    Slogs.append([])\n",
    "    pos = tor.rand((1, 2))*(UB-LB) + LB\n",
    "    vel = tor.zeros((1, 2))\n",
    "    S = tor.cat((pos, vel), 1)\n",
    "    Slogs[-1].append(S)\n",
    "    ret = 0\n",
    "    step = 0\n",
    "    while True:\n",
    "        # Take action\n",
    "        feat = actor_body(S)\n",
    "        mu = actor_mean(feat)\n",
    "        lsigma = actor_lsigma(feat)\n",
    "        try:\n",
    "            pol = tor.distributions.MultivariateNormal(mu, 0.01*tor.diag(tor.exp(lsigma[0])))\n",
    "        except:\n",
    "            print(\"A\")\n",
    "        A = pol.sample() # Don't use rsample() here\n",
    "        tor.clamp(A, ALB, AUB)\n",
    "\n",
    "        # Receive reward and next state\n",
    "        pos = pos + vel*dt + 0.5*A*dt**2\n",
    "        vel[pos < LB] = -0.1*vel[pos < LB]; vel[pos > UB] = -0.1*vel[pos > UB]\n",
    "        pos = tor.clamp(pos, LB, UB)\n",
    "        vel += A*dt\n",
    "        SP = tor.cat((pos, vel), 1)\n",
    "        R = -0.01\n",
    "        done = (tor.allclose(pos, tor.zeros(2), atol=0.25) and tor.allclose(vel, tor.zeros(2), atol=0.1)) #or step + 1 == n_timeout\n",
    "\n",
    "        # print(\"Step: {}, \".format(step))\n",
    "\n",
    "        # Learning\n",
    "        q = q_net(tor.cat((S, A), 1))\n",
    "        with tor.no_grad():\n",
    "          featP = actor_body(SP)\n",
    "          muP = actor_mean(featP)\n",
    "          lsigmaP = actor_lsigma(featP)\n",
    "          polP = tor.distributions.MultivariateNormal(muP, 0.01*tor.diag(tor.exp(lsigmaP[0])))\n",
    "          A2 = polP.sample()\n",
    "          q2 = q_net(tor.cat((SP, A2), 1));\n",
    "\n",
    "        # A.requires_grad = False\n",
    "        ## Q loss\n",
    "        qloss = (R + (1-done)*q2 - q)**2\n",
    "\n",
    "        # Policy loss\n",
    "        feat_pi = actor_body(S)\n",
    "        mu_pi = actor_mean(feat_pi)\n",
    "        lsigma_pi = actor_lsigma(feat_pi)\n",
    "        pol_pi = tor.distributions.MultivariateNormal(mu_pi, 0.01*tor.diag(tor.exp(lsigma_pi[0])))\n",
    "        A_pi = pol_pi.rsample()   # Requires rsample()\n",
    "        q_pi = q_net(tor.cat((S, A_pi), 1))\n",
    "        pobj = q_pi\n",
    "        ploss = -pobj\n",
    "\n",
    "\n",
    "        # A.requires_grad = True\n",
    "        popt.zero_grad()\n",
    "        ploss.backward()\n",
    "        popt.step()\n",
    "\n",
    "        qopt.zero_grad()\n",
    "        qloss.backward()\n",
    "        qopt.step()\n",
    "\n",
    "        # Log\n",
    "        Slogs[-1].append(SP)\n",
    "        ret += R\n",
    "        step += 1\n",
    "\n",
    "        # Termination\n",
    "        if done:\n",
    "            rets.append(ret)\n",
    "            i += 1\n",
    "            print(i, len(Slogs[-1]))\n",
    "            break\n",
    "        S = SP\n",
    "\n",
    "# Plotting\n",
    "plt.plot(-100*tor.tensor(rets))\n",
    "plt.figure()\n",
    "colors = [\"tab:blue\", \"tab:green\", \"tab:orange\", \"tab:purple\", \"tab:red\", \"tab:brown\"]\n",
    "for i in range(-min(30, EP), 0):\n",
    "    color = colors[i%len(colors)]\n",
    "    Slog = tor.cat(Slogs[i])\n",
    "    for i in range(Slog.shape[0]-1):\n",
    "        plt.plot(Slog[i:i+2,0], Slog[i:i+2,1], alpha=(i+1)/Slog.shape[0], color=color, marker='.')\n",
    "plt.xlim([LB[0, 0], UB[0, 0]])\n",
    "plt.ylim([LB[0, 1], UB[0, 1]])\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2613042d-ed2d-4790-9c18-24aab6bdb2f2",
   "metadata": {},
   "source": [
    "**AVG on Challenging Mujoco Benchmark Tasks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523780c2-11ed-4d64-b8f5-b3882524e71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, time\n",
    "import argparse, os, traceback\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import gymnasium as gym\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.distributions import MultivariateNormal\n",
    "from gymnasium.wrappers import NormalizeObservation\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def orthogonal_weight_init(m):\n",
    "    \"\"\" Orthogonal weight initialization for neural networks \"\"\"\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.orthogonal_(m.weight.data)\n",
    "        m.bias.data.fill_(0.0)\n",
    "\n",
    "def human_format_numbers(num, use_float=False):\n",
    "    # Make human readable short-forms for large numbers\n",
    "    magnitude = 0\n",
    "    while abs(num) >= 1000:\n",
    "        magnitude += 1\n",
    "        num /= 1000.0\n",
    "    # add more suffixes if you need them\n",
    "    if use_float:\n",
    "        return '%.2f%s' % (num, ['', 'K', 'M', 'G', 'T', 'P'][magnitude])\n",
    "    return '%d%s' % (num, ['', 'K', 'M', 'G', 'T', 'P'][magnitude])\n",
    "\n",
    "def set_one_thread():\n",
    "    '''\n",
    "    N.B: Pytorch over-allocates resources and hogs CPU, which makes experiments very slow!\n",
    "    Set number of threads for pytorch to 1 to avoid this issue. This is a temporary workaround.\n",
    "    '''\n",
    "    os.environ['OMP_NUM_THREADS'] = '1'\n",
    "    os.environ['MKL_NUM_THREADS'] = '1'\n",
    "    torch.set_num_threads(1)\n",
    "\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    \"\"\" Squashed Normal MLP \"\"\"\n",
    "    def __init__(self, obs_dim, action_dim, device, n_hid):\n",
    "        super(Actor, self).__init__()\n",
    "        self.device = device\n",
    "        self.LOG_STD_MAX = 2\n",
    "        self.LOG_STD_MIN = -20\n",
    "\n",
    "        # Two hidden layers\n",
    "        self.phi = nn.Sequential(\n",
    "            nn.Linear(obs_dim, n_hid),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(n_hid, n_hid),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "\n",
    "        self.mu = nn.Linear(n_hid, action_dim)\n",
    "        self.log_std = nn.Linear(n_hid, action_dim)\n",
    "\n",
    "        self.apply(orthogonal_weight_init)\n",
    "        self.to(device=device)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        phi = self.phi(obs.to(self.device))\n",
    "        phi = phi / torch.norm(phi, dim=1).view((-1, 1))\n",
    "        mu = self.mu(phi)\n",
    "        log_std = self.log_std(phi)\n",
    "        log_std = torch.clamp(log_std, self.LOG_STD_MIN, self.LOG_STD_MAX)\n",
    "\n",
    "        dist = MultivariateNormal(mu, torch.diag_embed(log_std.exp()))\n",
    "        action_pre = dist.rsample()\n",
    "        lprob = dist.log_prob(action_pre)\n",
    "        lprob -= (2 * (np.log(2) - action_pre - F.softplus(-2 * action_pre))).sum(axis=1)\n",
    "\n",
    "        # N.B: Tanh must be applied _only_ after lprob estimation of dist sampled action!!\n",
    "        #   A mistake here can break learning :/\n",
    "        action = torch.tanh(action_pre)\n",
    "        action_info = {'mu': mu, 'log_std': log_std, 'dist': dist, 'lprob': lprob, 'action_pre': action_pre}\n",
    "\n",
    "        return action, action_info\n",
    "\n",
    "\n",
    "class Q(nn.Module):\n",
    "    def __init__(self, obs_dim, action_dim, device, n_hid):\n",
    "        super(Q, self).__init__()\n",
    "        self.device = device\n",
    "\n",
    "        # Two hidden layers\n",
    "        self.phi = nn.Sequential(\n",
    "            nn.Linear(obs_dim + action_dim, n_hid),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(n_hid, n_hid),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        self.q = nn.Linear(n_hid, 1)\n",
    "        self.apply(orthogonal_weight_init)\n",
    "        self.to(device=device)\n",
    "\n",
    "    def forward(self, obs, action):\n",
    "        x = torch.cat((obs, action), -1).to(self.device)\n",
    "        phi = self.phi(x)\n",
    "        phi = phi / torch.norm(phi, dim=1).view((-1, 1))\n",
    "        return self.q(phi).view(-1)\n",
    "\n",
    "\n",
    "class AVG:\n",
    "    def __init__(self, cfg):\n",
    "        self.cfg = cfg\n",
    "        self.steps = 0\n",
    "\n",
    "        self.actor = Actor(obs_dim=cfg.obs_dim, action_dim=cfg.action_dim, device=cfg.device, n_hid=cfg.nhid_actor)\n",
    "        self.Q = Q(obs_dim=cfg.obs_dim, action_dim=cfg.action_dim, device=cfg.device, n_hid=cfg.nhid_critic)\n",
    "\n",
    "        self.popt = torch.optim.Adam(self.actor.parameters(), lr=cfg.actor_lr, betas=cfg.betas)\n",
    "        self.qopt = torch.optim.Adam(self.Q.parameters(), lr=cfg.critic_lr, betas=cfg.betas)\n",
    "\n",
    "        self.alpha, self.gamma, self.device = cfg.alpha_lr, cfg.gamma, cfg.device\n",
    "\n",
    "    def compute_action(self, obs):\n",
    "        obs = torch.Tensor(obs.astype(np.float32)).unsqueeze(0).to(self.device)\n",
    "        action, action_info = self.actor(obs)\n",
    "        return action, action_info\n",
    "\n",
    "    def update(self, obs, action, next_obs, reward, done, **kwargs):\n",
    "        obs = torch.Tensor(obs.astype(np.float32)).unsqueeze(0).to(self.device)\n",
    "        next_obs = torch.Tensor(next_obs.astype(np.float32)).unsqueeze(0).to(self.device)\n",
    "        action, lprob = action.to(self.device), kwargs['lprob']\n",
    "\n",
    "        #### Q loss\n",
    "        q = self.Q(obs, action.detach())    # N.B: Gradient should NOT pass through action here\n",
    "        with torch.no_grad():\n",
    "            next_action, action_info = self.actor(next_obs)\n",
    "            next_lprob = action_info['lprob']\n",
    "            q2 = self.Q(next_obs, next_action)\n",
    "            target_V = q2 - self.alpha * next_lprob\n",
    "\n",
    "        delta = reward + (1 - done) *  self.gamma * target_V - q\n",
    "        qloss = delta ** 2\n",
    "        ####\n",
    "\n",
    "        # Policy loss\n",
    "        ploss = self.alpha * lprob - self.Q(obs, action) # N.B: USE reparametrized action\n",
    "        self.popt.zero_grad()\n",
    "        ploss.backward()\n",
    "        self.popt.step()\n",
    "\n",
    "        self.qopt.zero_grad()\n",
    "        qloss.backward()\n",
    "        self.qopt.step()\n",
    "\n",
    "        self.steps += 1\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    tic = time.time()\n",
    "    run_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\") + f\"-{args.algo}-{args.env}_seed-{args.seed}\"\n",
    "\n",
    "    # Env\n",
    "    env = gym.make(args.env)\n",
    "    env = NormalizeObservation(env)\n",
    "\n",
    "    #### Reproducibility\n",
    "    env.reset(seed=args.seed)\n",
    "    env.action_space.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "    ####\n",
    "\n",
    "    # Learner\n",
    "    args.obs_dim =  env.observation_space.shape[0]\n",
    "    args.action_dim = env.action_space.shape[0]\n",
    "    agent = AVG(args)\n",
    "\n",
    "    # Interaction\n",
    "    rets, ep_steps = [], []\n",
    "    ret, step = 0, 0\n",
    "    terminated, truncated = False, False\n",
    "    obs, _ = env.reset()\n",
    "    ep_tic = time.time()\n",
    "    try:\n",
    "        for t in range(args.N):\n",
    "            # N.B: Action is a torch.Tensor\n",
    "            action, action_info = agent.compute_action(obs)\n",
    "            sim_action = action.detach().cpu().view(-1).numpy()\n",
    "\n",
    "            # Receive reward and next state\n",
    "            next_obs, reward, terminated, truncated, _ = env.step(sim_action)\n",
    "            agent.update(obs, action, next_obs, reward, terminated, **action_info)\n",
    "            ret += reward\n",
    "            step += 1\n",
    "\n",
    "            obs = next_obs\n",
    "\n",
    "            # Termination\n",
    "            if terminated or truncated:\n",
    "                rets.append(ret)\n",
    "                ep_steps.append(step)\n",
    "                print(\"E: {}| D: {:.3f}| S: {}| R: {:.2f}| T: {}\".format(len(rets), time.time() - ep_tic, step, ret, t))\n",
    "\n",
    "                ep_tic = time.time()\n",
    "                obs, _ = env.reset()\n",
    "                ret, step = 0, 0\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"Exiting this run, storing partial logs in the database for future debugging...\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "    if not (terminated or truncated):\n",
    "        # N.B: We're adding a partial episode just to make plotting easier. But this data point shouldn't be used\n",
    "        print(\"Appending partial episode #{}, length: {}, Total Steps: {}\".format(len(rets), step, t+1))\n",
    "        rets.append(ret)\n",
    "        ep_steps.append(step)\n",
    "\n",
    "    # Save returns and args before exiting run\n",
    "    if args.save_model:\n",
    "        agent.save(model_dir=args.results_dir, unique_str=f\"{run_id}_model\")\n",
    "\n",
    "\n",
    "    print(\"Run with id: {} took {:.3f}s!\".format(run_id, time.time()-tic))\n",
    "    return ep_steps, rets\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    args = parser.parse_args(args=[])\n",
    "\n",
    "\n",
    "    args.env = \"Hopper-v4\"\n",
    "    args.seed = 42\n",
    "    args.N = 10001000\n",
    "    args.actor_lr = 0.00006\n",
    "    args.critic_lr = 0.00087\n",
    "    args.gamma = 0.99\n",
    "    args.alpha_lr = 0.6\n",
    "    args.nhid_actor = 256\n",
    "    args.nhid_critic = 256\n",
    "    # Miscellaneous\n",
    "    args.results_dir = \"./results\"\n",
    "    parser.add_argument('--save_model', action='store_true', default=False)\n",
    "\n",
    "    # Adam\n",
    "    args.betas = [0, 0.999]\n",
    "\n",
    "    args.device = torch.device(\"cpu\")\n",
    "    args.algo = \"AVG\"\n",
    "\n",
    "    # Start experiment\n",
    "    set_one_thread()\n",
    "    main(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
